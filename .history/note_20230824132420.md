<center>WORD SHEET OF 
ML-PROBABILITY
==========
<font size=1>

*<center> 3220101842 Zhiyuan Zhu </center>*

<font size=3>

### Chapter 1&emsp;Introduction
- __1.2&emsp;Supervised Learning__

    convolutional neural network, CNN &emsp;卷积神经网络

    x are called the exogenous variables&emsp;外生变量

    y are called the endogenous variables&emsp;内生变量

    design matrix&emsp;设计矩阵

    tabular data&emsp;扁平数据

    featurization&emsp;特征化

    pair plot&emsp;二元关系

    dimensionality reduction

    decision boundary

    hypothetical asymmetric loss matrix&emsp;非对称损失矩阵

    misclassification rate&emsp;最小分类错误率

    indicator&emsp;指示函数

    zero-onr loss &emsp;0-1损失函数

    empirical risk minimization, ERM&emsp;经验风险最小化

    epistemic uncertainty&emsp;认知不确定性

    softmax function&emsp;归一化指数函数

    logit， sigmoid互为反函数

    affine function&emsp;仿射函数

    - statistics: &omega; parameters are usually called regression coefficients
    &ensp;b is called the intercept截距
    - ML: the parameters &omega; are called the weights 
    &ensp;b is called the bias.

    maximum likelihood estimate, MLE&emsp;极大似然估计

    quadratic loss&emsp;二次损失函数

    residuals&emsp;残差

    mean squared error, MSE&emsp;均方误差

    robust&emsp;扰动误差

    polynomial regression&emsp;多项式回归

    feature preprocessing, also called feature engineering&emsp;数据预处理

    interpolate&emsp;插值

    optimum&emsp;最佳

    feature extraction&emsp;特征提取

    deep neural networks, DNNs&emsp;深度神经网络

    convolutional neural networks (CNNs)&emsp;卷积神经网络, for images

    recurrent neural networks (RNNs)&emsp;循环神经网络, for sequences.

    underfitting&emsp;欠拟合

    overfitting&emsp;过拟合

    population risk&emsp;泛化误差？？？generalization error

    empirical risk&emsp;训练误差

    generalization gap&emsp;泛化鸿沟？？？

    test risk

    training set, test set and validation set&emsp;训练集，测试集，验证集

    inductive bias&emsp;归纳偏置

- __1.3&emsp;Unsupervised Learning__

    clusters&emsp;集群，聚类，簇

    latent factors&emsp;潜在因子

    factor analysis (FA)&emsp;因子分析

    variational auto-encoder&emsp;变分自编码器 VAE

    density estimation&emsp;密度估计

    data compression&emsp;数据压缩

    sample efficiency&emsp;样本效率

    interpretable&emsp;可解释的

- __1.4&emsp;Reinforcement Learning__

    reinforcement learning(RL)&emsp;强化学习

    SL: Learning with a teacher, tells you the best way to take

    RL: Learning with a critic, only thumbs up/down
- __1.5&emsp;Data__

    thresholding&emsp;阈值

    fine-grained classification&emsp;细粒度图像分类

    natural language processing or NLP &emsp;自然语言处理

    email spam classification, sentiment analysis&emsp;垃圾邮件分类

    to deal with discrete input, we have one-hot encoding, or dummy coding&emsp;独热编码

    feature crosses&emsp;特征交叉

    out-of-vocabulary, words appear in the test but not in the training data. OOV problem

    stop word&emsp;停用词

    word stemming&emsp;词干

    vector space model of text

    term frequency, TF&emsp;词频

    term frequency matrix&emsp;词频矩阵with size D&#215;N

    inverse document frequency, IDF&emsp;逆文件频率

    TF-IDF matrix:

    TFIDF<SUB>ij</SUB> = log(TF<SUB>ij</SUB> + 1) × IDF<SUB>i</SUB> 

    word embeddings&emsp;词嵌入

    byte-pair encoding, BPE&emsp;字节对编码

    missing completely at random or MCAR

    missing at random or MAR

    not missing at random or NMAR

    mean value imputation&emsp;缺失值处理

- __1.6&emsp;Discussion__

    predictive analytics&emsp;预测性分析

    caveats&emsp;警告

    alignment problem&emsp;对齐问题

    inverse reinforcement learning&emsp;逆向强化学习

    artificial general intelligence, AGI&emsp;通用人工智能



### Chapter 2&emsp;Probability: Univariate Models

- __2.1&emsp;Introduction__

    frequentist&emsp;频率学派

    epistemic uncertainty&emsp;认知不确定度

    model uncertainty&emsp;模型不确定性

    aleatoric uncertainty&emsp;偶然不确定性

    data uncertainty&emsp;数据不确定性

    Boolean logic&emsp;布尔逻辑

    joint probability&emsp;联合分布

    conditionally independent&emsp;条件独立

- __2.2&emsp;Random variables__

    random variable, rv&emsp;随机变量

    sample space, state space, X&emsp;样本空间

    discrete random variable&emsp;离散随机变量

    probability mass function, pmf&emsp;概率质量函数

    continuous random variable&emsp;连续随机变量

    cumulative distribution function, cdf&emsp;累积分布函数

    P for cdf, and Pr for probability

    Pr(a < X ≤ b) = P(b) − P(a)

    probability density function, pdf&emsp;概率密度函数

    pdf is derivative of cdf

    inverse cdf, percent point function, ppf, quantile function &emsp;分位函数

    quantile&emsp;分位

    median of distribution&emsp;中值

    lower and upper quartile&emsp;四分位数, 25%, 75%

    joint distribution&emsp;联合分布

    marginal distribution&emsp;边缘分布

    sun rule, the rule of total probability

    conditional distribution&emsp;条件概率分布

    product rule&emsp;乘法原理？？

    chain rule of probability&emsp;链式法则

    unconditionally independent, marginally independent&emsp;非条件独立, 边缘独立

    conditionally independent（CI）&emsp;条件独立

    graphical models&emsp;图模型

    mean, expect value, &mu;&emsp;期望

    linearity of expectation&emsp;期望的线性性

    variance, &sigma;<SUP>2</SUP>&emsp;方差

    standard derivation&emsp;标准差

    mode&emsp;众数

    x * = argmax p(x)

    multimodal&emsp;多峰的

    law of iterated expectations, law of total expectation&emsp;迭代期望定律，全期望公式

    law of total variance, conditional variance formula&emsp;方差分解公式, 条件方差公式

    inter-quartile range，IQR&emsp;四分卫距

- __2.3&emsp;Bayes's rule__

    Bayesian inference&emsp;贝叶斯推断

    prior distribution&emsp;先验分布

    observation distribution&emsp;观测分布

    likelihood&emsp;似然

    marginal likelihood&emsp;边缘似然函数

    posterior distribution&emsp;后验概率

    belief state&emsp;置信状态

    proportional to&emsp;成正比

    true positive rate&emsp;真阳率, sensitivity

    false negative rate&emsp;假阴率

    true negative rate&emsp;真阴率, specificity

    false positive rate&emsp;假阳率, 1-specificity

    prevalence&emsp;患病率

    Inverse problems&emsp;逆优化问题

- __2.4&emsp;Bernoulli and binomial distributions__

    Bernoulli distribution&emsp;Y ∼ Ber(θ)

    binomial distribution&emsp;二元分布

    sigmoid(logistic) function, S-shaped function

    heaviside step function&emsp;单位阶跃函数

    log odds&emsp;对数发生比&ensp; $p\over 1-p$

    The logistic function or sigmoid function maps the log-odds a to p

- __2.5&emsp;Categorical and multinomial distribution__

    categorical distribution&emsp;分类分布

    one-hot vector&emsp;独热编码, with C elements

    multinomial distribution&emsp;多项式分布

    temperature, divide each ${a_c}$ by a constant T called the temperature.

    - at low temperatures, the distribution puts most of its probability mass in the most probable state (winner takes all), 
    - at high temperatures, it spreads the mass uniformly

    multinomial logistic regression&emsp;多分类逻辑回归

    over-parameterized&emsp;过参数化

    log_sum-exp trick, avoid inf or nan, 

    lse function, which finds the largest exponents of a<sub>c</sub>

    cross-entropy loss&emsp;交叉熵损失函数

- __2.6&emsp;Univariate Gaussian (normal) distribution__

    Gaussian distribution&emsp;高斯分布，正态分布

    cumulative distribution function&emsp;累积分布函数, cdf

    probit function&emsp;分位函数

    homoscedastic regression&emsp;同方差回归

    heteroskedastic regression&emsp;异方差回归, c or k is all ok?

    linear regression&emsp;线性回归

    softplus function:&emsp;&sigma;<sub>+</sub> (a) = log (1 + e<sup> a</sup> )

    Dirac delta function&emsp;狄拉克 δ 函数

    useful property:&emsp;δ<sub>y</sub> (x) = δ (x − y)

    sifting property&emsp;筛选条件？？

- __2.7&emsp;Some other common univariate distributions__

    outliers&emsp;异类

    robust&emsp;专有名词，鲁棒

    Student t-distribution&emsp;t-分布

    degrees of freedom&emsp;自由度

    degree of normality&emsp;正态度？

    heavy tails&emsp;重尾分布，对outliers比较敏感

    usually we use &nu; = 4, which gives good performance in a range of problems

    Cauchy or Lorentz distribution, useful in Bayesian modeling, where we want to use a distribution over positive reals with heavy tails, but finite density at the origin

    Laplace distribution, double sided exponential distribution
    Beta distribution

    beta function: $B(a, b) = {Γ(a)Γ(b)\overΓ(a+b)}$

    Gamma function: $\Gamma(a) = \int_0^\infty x^{a−1}e^{-x}dx$

    Gamma distribution, shape a > 0 and the rate b > 0

    special case of gamma: 

    - Exponential distribution (shape = a = 1, rate = b = λ)
    - Chi-squared distribution
    - The inverse Gamma distribution
    
    Empirical distribution

    step function&emsp;阶跃函数

- __2.8&emsp;Transformations of random variables__

    Discrete case, Continuous case

    invertible transformations(bijection)&emsp;可逆变换

    change of variables formula&emsp;积分变量代换公式

    derive the mean and covariance:

    $\mathbb{E} [\pmb{y}] = \mathbb{E} [\pmb{Ax} + \pmb{b}] = \pmb{A}\mu + \pmb{b}$
    where $\mu = \mathbb{E} [\pmb{x}]$

    if $f(x) = \pmb{a^{T}x} + {b}$

    $\mathbb{E} [ \pmb{a^{T}x} + {b}] = \pmb{a^{T}\mu} + b $

    $Cov[\pmb{y}] = Cov[\pmb{Ax} + \pmb{b}] = \pmb{A \varSigma A^{T}}$

    where $\pmb{\varSigma} = Cov[\pmb{x}]$

    $\mathbb{V}[y] = \mathbb{V}[\pmb{a^{T}x} + b] = {a\varSigma a^{T}}$

    The convolution theorem&emsp;卷积定理

    differentiating under the integral sign&emsp;变限函数求导

    convolution operator: $\circledast$

    convolution theorem&emsp;卷积定理

    $p(y) = [{{d}\over{dy^{\*}}}  P_y(y^{\*})]_{y^{\*} = y} = \int p_1(x_1)p_2(y - x_1)dx_1$ 

    also written as $p = p_1 \circledast p_2$

    if $y = x_1 + x_2$

    then $p(y) = \mathcal{N}(x_1 | \pmb{\mu}_1, {\sigma}_1^{2}) \bigotimes \mathcal{N}(x_2 | \pmb{\mu}_2, {\sigma}_2^{2})$ = $\mathcal{N}(y | \pmb{\mu}_1 + \pmb{\mu}_2, {\sigma}_1^{2} + {\sigma}_2^{2})$

    the convolution of two Gaussians is a Gaussian

    Central limit theorem&emsp;中心极限定理

    independent and identically distributed, iid&emsp;独立同分布，每个变量的概率分布都相同，且这些随机变量互相独立

    Monte Carlo approximation&emsp;Monte Carlo近似


### Chapter 3&emsp;Probability: Multivariate Models
- __3.1&emsp;Joint distributions for multiple random variables__

    covariance&emsp;协方差

    $\mathbb{E}[x x^{T}] = \varSigma + \mu\mu^{T}$

    $Cov[\pmb{Ax}+\pmb{b}] = \pmb{A}Cov[\pmb{x}]\pmb{A}^{T}$

    covariance matrix is defined to be a symmetric, positive semi definite matrix

    cross-covariance&emsp;互协方差

    $Cov[\pmb{x},\pmb{y}] = \mathbb{E}[(\pmb{x} - \mathbb{E}[\pmb{x}])(\pmb{y} - \mathbb{E}[\pmb{y}]^{T})]$

    The (Pearson) correlation coefficient&emsp;Pearson 相关系数

    $-1 \leq \rho \leq +1$

    $\rho \triangleq corr[X, Y] \triangleq {Cov[X,Y]\over{\sqrt{\mathbb{V}[X]\mathbb{V}[Y]}}}$

    corr [X, Y] = 1 if and only if Y = aX + b (and a > 0)  i.e., if there is a linear relationship between X and Y

    correlation matrix&emsp;协方差矩阵

    $corr(\pmb{x}) = (diag(\pmb{K}\_{xx}))^{-\frac1 2} \pmb{K}\_{xx} (diag(\pmb{K}\_{xx}))^{-\frac1 2}$

    对角元素已经归一化

    $\pmb{K}_{xx}$ is the auto-covariance matrix&emsp;自协方差矩阵

    $\pmb{K}\_{xx} = \varSigma = \mathbb{E}[(\pmb{x} - \mathbb{E}[\pmb{x}])(\pmb{x} - \mathbb{E}[\pmb{x}])^{\pmb{T}}] = \pmb{R}\_{xx} - \mu \mu^{T}$

    $\pmb{R}_{xx} = \mathbb{E}[xx^{T}]$ is the autocorrelation matrix&emsp;自相关矩阵

    __Note:__
    - Uncorrelated does not imply independent!

    不独立$\neq$不相关

    - Correlation does not imply causation!

    相关不蕴含因果

    spurious correlation&emsp;伪相关

    Simpson’s paradox&emsp;Simpson悖论

- __3.2&emsp;The multivariate Gaussian (normal) distribution__

    the multivariate Gaussian or multivariate normal (MVN)&emsp;多元高斯分布
    
    $\mathbb{E}[\pmb{y\ y}^{T}] = \varSigma + \mu\mu^{T}$

    normalization constant Z, ensures th epdf integrates to 1

    the bivariate Gaussian distribution&emsp;二维正态分布

    correlation coefficient&emsp;相关系数

    full covariance matrix, D(D + 1)/2 parameters&emsp;完全，协方差矩阵

    diagonal covariance matrix, diagonal covariance matrix&emsp;对角，协方差矩阵

    spherical covariance matrix, isotropic covariance matrix,&emsp;球面协方差矩阵，各向同性

    Mahalanobis distance&emsp;马氏距离

    $\Delta^{2} \triangleq (\pmb{y} - \pmb{\mu})^{T}\pmb{\Sigma}^{-1}(\pmb{y} - \pmb{\mu})$

    eigendecomposition&emsp;矩阵特征分解

    $(\pmb{y}-\pmb{y})^{T}\pmb{\Sigma}^{-1}(\pmb{y} - \pmb{\mu})$
